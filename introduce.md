# 蒙特卡洛树搜索：博弈 AI 的“直觉”与“理性”

如果你曾好奇 AI 是如何在复杂如围棋、象棋的博弈中寻找“最优解”的，那么 MCTS 就是你绕不开的必修课。它不靠暴力穷举，也不完全依赖预设的经验，而是在**探索未知**与**利用已知**之间，寻找一种优雅的平衡。

------

## 核心直觉：如果不知道怎么走，就多试几次

传统的博弈算法（如 Minimax）像是一个严谨的数学家，试图算清未来的每一步；而 MCTS 更像是一个**经验丰富的探险家**。

面对成千上万种可能，MCTS 的核心思想很简单：通过大量的随机模拟，把概率变成胜率。

就像你在一个从未去过的森林里找出口，与其在原地纠结地图，不如先随机往各个方向跑几次，看看哪条路最后通向了阳光。

------

## MCTS 的四大生命周期

MCTS 的每一次决策，都经历着四个循环往复的阶段：

1. **选择 (Selection)**：从根节点开始，按照某种“聪明”的策略（如 UCB1）向下移动，直到找到一个尚未完全展开的节点。
2. **扩展 (Expansion)**：在这个节点下增加一个或多个子节点，代表博弈中下一步的可能性。
3. **仿真 (Simulation / Rollout)**：从新节点开始，进行“随机演习”。AI 会自己跟自己快速下完这局棋（通常是随机走位），直到分出胜负。
4. **回溯 (Backpropagation)**：将这局演习的结果（赢或输）顺着路径传回根节点，更新沿途所有节点的“身价”（胜率信息）。

------

## ⚖关键秘诀：UCB1 公式

MCTS 之所以“聪明”，是因为它解决了博弈中最大的难题：**我是该尝试那些还没试过的（探索），还是坚持目前看起来最好的（利用）？**

这就是著名的 **Upper Confidence Bound (UCB1)** 公式：

$$Score = \bar{X}_j + C \sqrt{\frac{\ln N}{n_j}}$$

- **$\bar{X}_j$ (Exploitation)**：当前的平均胜率。胜率越高，越值得继续走。
- **$\sqrt{\frac{\ln N}{n_j}}$ (Exploration)**：探索潜力。这个动作被试过的次数 $n_j$ 越少，它的潜力值就越高。
- **$C$**：一个平衡常数。

> **白话解读**：公式左边是“眼前的利益”，右边是“对未知的好奇”。$C$ 就是你的性格：$C$ 越大，AI 越喜欢冒险；$C$ 越小，AI 越保守。

------

## 为什么 MCTS 改变了游戏规则？

- **无需先验知识**：它不需要你告诉它“围棋中占角很重要”。只要给它规则，它就能通过自我博弈进化。
- **非对称增长**：它不会平铺直叙地搜索，而是会把算力集中在那些“更有希望”的分支上。
- **随时停止**：你可以给它 1 秒思考，也可以给它 1 小时。无论何时停止，它都能给你当前算出的最优解。

